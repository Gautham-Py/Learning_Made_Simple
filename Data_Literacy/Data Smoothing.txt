> Data Smoothing
                Data Smoothing refers to statistical approach of eliminating outliers from datasets to make patterns more noticeable. 
                It is done using algorithms to eliminate  statistical noise from datasets.
                Ex: Forecasts patterns in share prices.

  Methods:
  
  ---> Simple Exponential: 
                 It uses average calculation for assigning the exponentially declining weights 
                 beginning with most recent observation.

                > Not capable of managing trends well. (Hence used when cyclical variations
                  are not present)

  ---> Moving Average:
                 Best when there is slight or no Seasonal variation.
                 Used for seperating random variation.

  ---> Random Walk:
                 Commonly used for describing the patterns in financial instruments.
                 > Assumption: a random variable will give the potential data points when 
                               added to the last accessible data point.

  ---> Exponential Moving Average:
                  Weights are applied to historical observations, after using the exponential smoothing method.
                  > Responds to faster to price changes.


  Limitations:
      > Data smoothing does not necessarily offer an interpretation of the themes or patterns  
        it helps to recognize.

      > It may contribute to certain data points being overlooked by focusing on others.

      > May eliminate the usable data points, which may lead to incorrect forecasts if the
        data set is seasonal and not completely be reflective of the reality produced by the data points.

      > Data smoothing can be prone to considerable disruption from the outliers in the
        data.          
                